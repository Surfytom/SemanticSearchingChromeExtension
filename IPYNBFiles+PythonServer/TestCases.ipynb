{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the first 4 blocks of code then choose which test case block you want to run.\n",
    "import weaviate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(   \n",
    "    url=\"http://localhost:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes': [{'class': 'Paper', 'description': 'Articles', 'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2}, 'cleanupIntervalSeconds': 60, 'stopwords': {'additions': None, 'preset': 'en', 'removals': None}}, 'moduleConfig': {'text2vec-transformers': {'poolingStrategy': 'masked_mean', 'vectorizeClassName': True}}, 'properties': [{'dataType': ['string'], 'description': 'The id', 'indexInverted': False, 'moduleConfig': {'text2vec-transformers': {'skip': True, 'vectorizePropertyName': False}}, 'name': 'pdfId', 'tokenization': 'word'}, {'dataType': ['text'], 'description': 'The abstract', 'indexInverted': True, 'moduleConfig': {'text2vec-transformers': {'skip': False, 'vectorizePropertyName': False}}, 'name': 'abstract', 'tokenization': 'word'}, {'dataType': ['string[]'], 'description': 'The categories', 'indexInverted': False, 'moduleConfig': {'text2vec-transformers': {'options': {'useCache': True, 'useGPU': True, 'waitForModel': True}, 'skip': True, 'vectorizePropertyName': False}}, 'name': 'categories', 'tokenization': 'word'}, {'dataType': ['int'], 'description': 'The year of the most recent version of the paper', 'indexInverted': False, 'moduleConfig': {'text2vec-transformers': {'skip': True, 'vectorizePropertyName': False}}, 'name': 'version'}, {'dataType': ['text'], 'description': \"This property was generated by Weaviate's auto-schema feature on Tue Apr 18 17:49:56 2023\", 'moduleConfig': {'text2vec-transformers': {'skip': False, 'vectorizePropertyName': False}}, 'name': 'title', 'tokenization': 'word'}], 'replicationConfig': {'factor': 1}, 'shardingConfig': {'virtualPerPhysical': 128, 'desiredCount': 1, 'actualCount': 1, 'desiredVirtualCount': 128, 'actualVirtualCount': 128, 'key': '_id', 'strategy': 'hash', 'function': 'murmur3'}, 'vectorIndexConfig': {'skip': False, 'cleanupIntervalSeconds': 300, 'maxConnections': 64, 'efConstruction': 128, 'ef': -1, 'dynamicEfMin': 100, 'dynamicEfMax': 500, 'dynamicEfFactor': 8, 'vectorCacheMaxObjects': 1000000000000, 'flatSearchCutoff': 40000, 'distance': 'cosine', 'pq': {'enabled': False, 'bitCompression': False, 'segments': 0, 'centroids': 256, 'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}}, 'vectorIndexType': 'hnsw', 'vectorizer': 'text2vec-transformers'}]}\n"
     ]
    }
   ],
   "source": [
    "# Verifies the datbase is connected. Should print a schem with the class Paper if using the projects database\n",
    "print(client.schema.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyPrintResults (queryResults):\n",
    "    # Prints nicer looking results that outputting the raw json\n",
    "\n",
    "    bold = '\\033[1m'\n",
    "    end = '\\033[0m'\n",
    "\n",
    "    results = queryResults[\"data\"][\"Get\"][\"Paper\"]\n",
    "\n",
    "    for paper in results:\n",
    "\n",
    "        title = bold + \"Title\" + end + \"\\n\\n\" + paper[\"title\"] + \"\\n\"\n",
    "        certainty = bold + \"Certainty\" + end + \"\\n\\n\" + str(paper[\"_additional\"][\"certainty\"]) + \"\\n\"\n",
    "        abstract = bold + \"Abstract\" + end + \"\\n\\n\" + paper[\"abstract\"] + \"\\n\"\n",
    "\n",
    "        resultPdfLink = paper[\"pdfId\"]\n",
    "\n",
    "        if (paper[\"pdfId\"].__contains__(\"/\") is False):\n",
    "            resultPdfLink = resultPdfLink.replace(\"-\", \".\");\n",
    "    \n",
    "        resultPdfLink = \"https://arxiv.org/pdf/\" + resultPdfLink + \".pdf\";\n",
    "\n",
    "        link = bold + \"Link\" + end + \"\\n\\n\" + resultPdfLink + \"\\n\"\n",
    "\n",
    "        print(title)\n",
    "        print(certainty)\n",
    "        print(abstract)\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Latent Dirichlet Allocation (LDA) and Topic modeling: models,   applications, a survey\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.8717871308326721\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data, text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modeling, which Latent Dirichlet allocation (LDA) is one of the most popular methods in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper can be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated scholarly articles highly (between 2003 to 2016) related to Topic Modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. Also, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/1711.04305.pdf\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "AI supported Topic Modeling using KNIME-Workflows\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.8658085465431213\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  Topic modeling algorithms traditionally model topics as list of weighted terms. These topic models can be used effectively to classify texts or to support text mining tasks such as text summarization or fact extraction. The general procedure relies on statistical analysis of term frequencies. The focus of this work is on the implementation of the knowledge-based topic modelling services in a KNIME workflow. A brief description and evaluation of the DBPedia-based enrichment approach and the comparative evaluation of enriched topic models will be outlined based on our previous work. DBpedia-Spotlight is used to identify entities in the input text and information from DBpedia is used to extend these entities. We provide a workflow developed in KNIME implementing this approach and perform a result comparison of topic modeling supported by knowledge base information to traditional LDA. This topic modeling approach allows semantic interpretation both by algorithms and by humans. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/2104.09428.pdf\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "TopicSifter: Interactive Search Space Reduction Through Targeted Topic   Modeling\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.8650010526180267\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or \"targets\" rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/1907.12079.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test case one\n",
    "\n",
    "testArgument = {\n",
    "    \"concepts\": [\"Topic modeling can help in optimizing the search process. In this article, we will be discussing Latent Dirichlet Allocation, a topic modeling process.\"]\n",
    "}\n",
    "\n",
    "data = client.query.get(\"Paper\", [\"pdfId title abstract _additional {certainty}\"]).with_near_text(testArgument).with_limit(3).do()\n",
    "\n",
    "prettyPrintResults(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Object-Oriented Parallel Programming\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.8802706003189087\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  We introduce an object-oriented framework for parallel programming, which is based on the observation that programming objects can be naturally interpreted as processes. A parallel program consists of a collection of persistent processes that communicate by executing remote methods. We discuss code parallelization and process persistence, and explain the main ideas in the context of computations with very large data objects. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/1404.4666.pdf\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Effective Parallelisation for Machine Learning\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.8790191411972046\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on the efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/1810.03530.pdf\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Automatic Parallelization: Executing Sequential Programs on a Task-Based   Parallel Runtime\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.8785797953605652\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  There are billions of lines of sequential code inside nowadays' software which do not benefit from the parallelism available in modern multicore architectures. Automatically parallelizing sequential code, to promote an efficient use of the available parallelism, has been a research goal for some time now. This work proposes a new approach for achieving such goal. We created a new parallelizing compiler that analyses the read and write instructions, and control-flow modifications in programs to identify a set of dependencies between the instructions in the program. Afterwards, the compiler, based on the generated dependencies graph, rewrites and organizes the program in a task-oriented structure. Parallel tasks are composed by instructions that cannot be executed in parallel. A work-stealing-based parallel runtime is responsible for scheduling and managing the granularity of the generated tasks. Furthermore, a compile-time granularity control mechanism also avoids creating unnecessary data-structures. This work focuses on the Java language, but the techniques are general enough to be applied to other programming languages. We have evaluated our approach on 8 benchmark programs against OoOJava, achieving higher speedups. In some cases, values were close to those of a manual parallelization. The resulting parallel code also has the advantage of being readable and easily configured to improve further its performance manually. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/1604.03211.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test case two\n",
    "\n",
    "testArgument = {\n",
    "    \"concepts\": [\"Parallelism is essential to effective use of accelerators because they contain many independent processing elements that are capable of executing code in parallel. There are three ways to develop parallel code.\"]\n",
    "}\n",
    "\n",
    "data = client.query.get(\"Paper\", [\"pdfId title abstract _additional {certainty}\"]).with_near_text(testArgument).with_limit(3).do()\n",
    "\n",
    "prettyPrintResults(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Intrinsic cell factors that influence tumourigenicity in cancer stem   cells - towards hallmarks of cancer stem cells\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.7861415147781372\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  Since the discovery of a cancer initiating side population in solid tumours, studies focussing on the role of so-called cancer stem cells in cancer initiation and progression have abounded. The biological interrogation of these cells has yielded volumes of information about their behaviour, but there has, as of yet, not been many actionable generalised theoretical conclusions. To address this point, we have created a hybrid, discrete/continuous computational cellular automaton model of a generalised stem-cell driven tissue and explored the phenotypic traits inherent in the inciting cell and the resultant tissue growth. We identify the regions in phenotype parameter space where these initiating cells are able to cause a disruption in homeostasis, leading to tissue overgrowth and tumour formation. As our parameters and model are non-specific, they could apply to any tissue cancer stem-cell and do not assume specific genetic mutations. In this way, our model suggests that targeting these phenotypic traits could represent generalizable strategies across cancer types and represents a first attempt to identify the hallmarks of cancer stem cells. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/1301.3934.pdf\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Bone Marrow Cell Recognition: Training Deep Object Detection with A New   Loss Function\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.7207980453968048\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  For a long time, bone marrow cell morphology examination has been an essential tool for diagnosing blood diseases. However, it is still mainly dependent on the subjective diagnosis of experienced doctors, and there is no objective quantitative standard. Therefore, it is crucial to study a robust bone marrow cell detection algorithm for a quantitative automatic analysis system. Currently, due to the dense distribution of cells in the bone marrow smear and the diverse cell classes, the detection of bone marrow cells is difficult. The existing bone marrow cell detection algorithms are still insufficient for the automatic analysis system of bone marrow smears. This paper proposes a bone marrow cell detection algorithm based on the YOLOv5 network, trained by minimizing a novel loss function. The classification method of bone marrow cell detection tasks is the basis of the proposed novel loss function. Since bone marrow cells are classified according to series and stages, part of the classes in adjacent stages are similar. The proposed novel loss function considers the similarity between bone marrow cell classes, increases the penalty for prediction errors between dissimilar classes, and reduces the penalty for prediction errors between similar classes. The results show that the proposed loss function effectively improves the algorithm's performance, and the proposed bone marrow cell detection algorithm has achieved better performance than other cell detection algorithms. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/2110.12647.pdf\n",
      "\n",
      "\u001b[1mTitle\u001b[0m\n",
      "\n",
      "Modeling adult skeletal stem cell response to laser-machined   topographies through deep learning\n",
      "\n",
      "\u001b[1mCertainty\u001b[0m\n",
      "\n",
      "0.7119418680667877\n",
      "\n",
      "\u001b[1mAbstract\u001b[0m\n",
      "\n",
      "  The response of adult human bone marrow stromal stem cells to surface topographies generated through femtosecond laser machining can be predicted by a deep neural network. The network is capable of predicting cell response to a statistically significant level, including positioning predictions with a probability P < 0.001, and therefore can be used as a model to determine the minimum line separation required for cell alignment, with implications for tissue structure development and tissue engineering. The application of a deep neural network, as a model, reduces the amount of experimental cell culture required to develop an enhanced understanding of cell behavior to topographical cues and, critically, provides rapid prediction of the effects of novel surface structures on tissue fabrication and cell signaling. \n",
      "\n",
      "\u001b[1mLink\u001b[0m\n",
      "\n",
      "https://arxiv.org/pdf/2006.00248.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test case three\n",
    "\n",
    "testArgument = {\n",
    "    \"concepts\": [\"Stem cells are the body's raw materials â€” cells from which all other cells with specialized functions are generated. Under the right conditions in the body or a laboratory, stem cells divide to form more cells called daughter cells.\"]\n",
    "}\n",
    "\n",
    "data = client.query.get(\"Paper\", [\"pdfId title abstract _additional {certainty}\"]).with_near_text(testArgument).with_limit(3).do()\n",
    "\n",
    "prettyPrintResults(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonGUI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
